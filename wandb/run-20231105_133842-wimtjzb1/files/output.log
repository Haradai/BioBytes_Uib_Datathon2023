Attentive_model(
  (embedding): Embedding(1360, 1024)
  (selfattn1): AttentionBlock(
    (key_gen): Sequential(
      (0): Linear(in_features=18, out_features=80, bias=True)
      (1): ReLU()
      (2): Linear(in_features=80, out_features=80, bias=True)
      (3): ReLU()
      (4): Linear(in_features=80, out_features=1024, bias=True)
      (5): ReLU()
    )
    (val_gen): Sequential(
      (0): Linear(in_features=1024, out_features=80, bias=True)
      (1): ReLU()
      (2): Linear(in_features=80, out_features=80, bias=True)
      (3): ReLU()
      (4): Linear(in_features=80, out_features=1024, bias=True)
      (5): ReLU()
    )
    (query_gen): Sequential(
      (0): Linear(in_features=18, out_features=80, bias=True)
      (1): ReLU()
      (2): Linear(in_features=80, out_features=80, bias=True)
      (3): ReLU()
      (4): Linear(in_features=80, out_features=1024, bias=True)
      (5): ReLU()
    )
    (multihead_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (rel): ReLU()
  (fc_1): Linear(in_features=1024, out_features=1024, bias=True)
  (fc_regr): Linear(in_features=1024, out_features=1, bias=True)
)
[34m[1mwandb[39m[22m: Currently logged in as: [33mharadai[39m. Use [1m`wandb login --relogin`[22m to force relogin